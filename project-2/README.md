# ðŸ  House Price Prediction â€” Linear Regression

A supervised Machine Learning project that predicts California house prices using **Linear Regression**. Built with Python in Google Colab, this project covers the full ML pipeline â€” from data loading and exploration to model training, evaluation, and feature engineering.

---

## ðŸ“Œ Project Overview

| Item | Details |
|------|---------|
| **Type** | Supervised Machine Learning â€” Regression |
| **Algorithm** | Linear Regression |
| **Dataset** | California Housing (built into `sklearn`) |
| **Target Variable** | House Price (in $100,000s) |
| **Features** | 8 (MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude) |
| **Dataset Size** | 20,640 rows Ã— 9 columns |
| **Platform** | Google Colab |

---

## ðŸ§° Tech Stack

- **Python 3**
- **NumPy** â€” numerical computations
- **Pandas** â€” data manipulation
- **Matplotlib** â€” data visualization
- **Scikit-learn** â€” ML model, dataset, and metrics

---

## ðŸ“ Project Structure

```
house-price-prediction/
â”‚
â”œâ”€â”€ Project_2.ipynb                  # Main Jupyter Notebook
â”œâ”€â”€ house_price_predictions.csv      # Model output (Actual vs Predicted)
â””â”€â”€ README.md                        # Project documentation
```

---

## ðŸ”„ ML Pipeline (Step-by-Step)

### Step 1 â€” Import Libraries
```python
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
```

### Step 2 â€” Load Dataset
Uses sklearn's built-in **California Housing** dataset â€” no download or URL required.

### Step 3 â€” Data Understanding
- Shape: `(20640, 9)` â€” 20,640 records, 9 columns
- All features are `float64`, zero null values

### Step 4 â€” Check Missing Values
No missing values found across all 9 columns.

### Step 5 â€” Feature / Target Split
```python
X = df.drop("Price", axis=1)   # 8 input features
y = df["Price"]                 # Target: house price
```

### Step 6 â€” Train-Test Split
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# Training: 16,512 samples | Testing: 4,128 samples
```

### Step 7 â€” Train the Model
```python
model = LinearRegression()
model.fit(X_train, y_train)
```

### Step 8 â€” Make Predictions
```python
y_pred = model.predict(X_test)
```

### Step 9 â€” Evaluate the Model
| Metric | Score |
|--------|-------|
| **RMSE** | 0.7456 |
| **RÂ² Score** | 0.5758 |

> **RMSE** â€” average prediction error (lower = better)  
> **RÂ²** â€” how well the model explains variance in the data (closer to 1 = better)

### Step 10 â€” Visualization: Actual vs Predicted
Scatter plot showing predicted prices against actual prices. Dots close to the diagonal line indicate better predictions.

### Step 11 â€” Residual Plot (Error Analysis)
Scatter plot of residuals (`actual âˆ’ predicted`) vs predicted values. A good model shows residuals randomly scattered around zero.

### Step 12 â€” Feature Importance (Coefficients)
| Feature | Coefficient | Impact |
|---------|------------|--------|
| AveBedrms | +0.783 | â†‘ Increases price |
| MedInc | +0.449 | â†‘ Increases price |
| HouseAge | +0.010 | â†‘ Slight increase |
| Population | âˆ’0.000002 | â†“ Negligible |
| AveOccup | âˆ’0.004 | â†“ Slight decrease |
| AveRooms | âˆ’0.123 | â†“ Decreases price |
| Latitude | âˆ’0.420 | â†“ Decreases price |
| Longitude | âˆ’0.434 | â†“ Decreases price |

### Step 13 â€” Feature Engineering (Log Transform)
Applied `np.log1p()` on the target variable to reduce skewness.

| Metric | Before Log | After Log |
|--------|-----------|----------|
| RMSE | 0.7456 | **0.2244** |
| RÂ² Score | 0.5758 | **0.6006** |

### Step 14 â€” Save Output
Predictions saved to `house_price_predictions.csv` with `Actual` and `Predicted` columns.

---

## ðŸ“Š Key Results

- The baseline Linear Regression model achieved **RÂ² = 0.576**, meaning it explains ~57.6% of price variance.
- After applying a **log transformation** on the target, the model improved to **RÂ² = 0.601**.
- The most influential feature is **AveBedrms** (positive) and **Longitude** (negative).

---

## ðŸš€ How to Run

1. Open `Project_2.ipynb` in [Google Colab](https://colab.research.google.com/) or Jupyter Notebook
2. Run all cells top to bottom
3. No dataset download needed â€” uses `sklearn.datasets.fetch_california_housing`

**Install dependencies (if running locally):**
```bash
pip install numpy pandas matplotlib scikit-learn
```

---

## ðŸ§  Concepts Demonstrated

- Supervised learning with Linear Regression
- Train-test split strategy (`80/20`)
- Regression metrics: RMSE and RÂ² Score
- Feature importance via model coefficients
- Feature engineering with log transformation
- Visualization: Actual vs Predicted & Residual plots
- Saving predictions to CSV

---

## ðŸ™‹ Author

**Your Name**  
[GitHub](https://github.com/DeveloperSomnath) â€¢ [LinkedIn](https://www.linkedin.com/in/somnath-das-02527a26b/)

---

## ðŸ“„ License

This project is open source and available under the [MIT License](LICENSE).
